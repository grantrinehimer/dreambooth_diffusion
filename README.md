# A Reimplementation of _DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation_

<!-- ## Environment

Try to use the environment.yaml to make a conda environment. Honestly, this might not work so if not just make sure you're using python 3.10 and install packages with pip as needed. You could also try the requirements.txt as well (although diffusers has to be installed manually through cloning the repo to your computer). -->

## Introduction
This GitHub repository contains our reimplementation of the DreamBooth architecture described in “DreamBooth: Fine-Tuning Text-to-Image Diffusion Models for Subject-Driven Generation” (Ruiz et al., 2023)[1]. Recent text-to-image (T2I) models have demonstrated promising performance in generating high quality images given a text prompt. However, these models lack the ability to generate accurate and consistent renditions of a particular subject in different contexts that correspond with textual input after being fine-tuned or trained on a reference set of specific subjects. Dreambooth personalizes pre-trained T2I models by fine-tuning them on just 3–5 images of a subject, using a "rare" token identifier and class-specific prompts to generate high-quality images that preserves subjects with consitency and high fidelity in a diverse set of contexts indicated by the textual input. The Dreambooth paper [1] utilizes a class-specific Prior Preservation Loss (PPL) that allows T2I models to generate a diverse and contextually appropriate image of a subject while effectively preserving its identity.

## Chosen Result
We extend Figure 12 and Table 3 from the original paper [1] to qualitatively compare our reimplementation [Figure 1] and quantitatively assess the effect of PPL on subject fidelity and output diversity—key factors in evaluating the success of subject-driven generation. 
## GitHub Contents

```
├── data/                     # Real reference images and metadata
│   ├── subjects.csv          # subject_name,class,live flag
|   └── ppl/                  # Location to store ppl images for fine-tuning (generated by batch_dreambooth)
│       └── <subject_name>/…
|   └── subjects/             # real image folders (e.g. data/subjects/dog/00.jpg,...)
│       └── <subject_name>/…
│
├── results/                  # Generated images by condition
│   ├── no_ppl/               # baseline generations (no prior preservation)
│   │   └── <subject_name>/…  
│   └── ppl/                  # generations with Prior Preservation Loss
│       └── <subject_name>/…
│
├── code/                     # Evaluation scripts
|   ├── metrics/              # Metric scripts
│   │   └── pres.py           # computes PRES metric (and DINO metric)!
│   │   └── div.py            # computes DIV metric
│   │   └──clip_embeddings.py # computes CLIP‑I and CLIP‑T
│   └── evaluation.ipynb      # notebook to aggregate & export metrics
│   └── dreambooth_finetune/  # Code derived directly from [2] and adapted for use within batch_dreambooth
│   └── batch_config.yaml     # Configuration for directories to place results, models, and data
│   └── training_config.yaml     # Contains training configurations including pretrained model location
│   └── stable_diffusion.ipynb   # Code for downloading pretrained model
│   └── dreambooth_test.ipynb    # Example code for running inference on a fine-tuned model
│   └── batch_dreamboth.py       # Training script to generate models and results on all subjects
│   └── stable_diffusion.ipynb   # Code for downloading pretrained model


├── requirements.txt          # Python dependencies
└── README.md                 # this file
```

## Re-implementation Details

This repository contains code and evaluation scripts to reproduce the quantitative results of our DreamBooth reimplementation (Stable Diffusion v1.5) and compare them against the original DreamBooth (Imagen) metrics. The evaluation produces per-class and per-condition (no‐PPL vs. PPL) scores for PRES, DIV, DINO, CLIP-I, and CLIP-T, and outputs summary CSV tables and an averaged comparison.

In our reimplementation, we finetuned a pretrained Stable Diffusion v1.5 T2I model. The original paper used Imagen (proprietary) and Stable Diffusion (the version was not disclosed). We used the paper’s original dataset [1], consisting of 30 subjects and 15 classes. The original paper used 25 prompts and generated 4 images per subject and prompt. We used a subset of 8 prompts and generated 2 images per subject and a prompt for computational feasibility.

We utilized a T4 GPU with 16 GB of GPU memory hosted on Google Cloud (with preinstalled CUDA 12.6). We finetuned two models for each subject, one using PPL and one without. For PPL, we generated 100 class samples before finetuning, rather than the 1000 used in the paper (although the paper noted that 
less could be used). We also trained each model over 400 training steps rather than 1000. On testing, we found this did not change the quality of results. We also used a variety of techniques to solve memory issues. We used an FP16 quantized pretrained model, FP16 mixed precision training, an 8-bit Adam optimizer, and gradient checkpointing. The extent to which these methods are used in the paper isn’t clear.

We made use of a Huggingface dreambooth CLIP training script [2] for our implementation and adapted it for use in our script to train models and run inference on all 30 subjects. Our script allows config files to be passed to generate results on any number of subjects in a batched format. We wrote scripts to compute CLIP and DINO embeddings on our results to recreate the main table result of the paper. We also used our own prompts to recreate the various subject adaptation images in the paper. 

For evaluation, we computed four complementary metrics to assess the performance of our fine-tuned models under both the no-PPL (baseline) and PPL conditions. Our calculated scores for these metrics were compared with the original paper’s scores [1] present in their Table 3. We measured subject fidelity to quantify how faithfully our fine-tuned models reproduced the unique visual identity of the reference subject via two metrics: the first is DINO, which found the average cosine similarity between ViT-S/8 DINO embeddings of real and generated images. This differs from Dreambooth’s [1] calculation of DINO (and all other metrics using ViTs pretrained with the DINO self-supervised learning method [3]), as Ruiz et al. utilize ViT-S/16 DINO instead. The second subject fidelity metric is CLIP-I, standing for the average cosine similarity between CLIP embeddings of real and generated images In addition, we measure prompt fidelity to see how well a generated image matches its conditioning text prompt, done through CLIP-T: the average cosine similarity between CLIP prompt text embeddings and generated image embeddings from that same prompt.

We evaluated prior collapse through the PRES (preservation) metric, found by the average cosine similarity of ViT-S/8 DINO embeddings between real images of a subject and generated images of other subjects in the same class. The last metric computed was DIV, representing the diversity between generated samples of the same prompt. We quantify DIV by computing the mean LPIPS distance between all pairs of generated images sharing the same prompt and then averaging those per-prompt scores across the full prompt set. Concretely, for each subject, we took its few real reference images and its generated outputs, extracted normalized embeddings, formed all pairwise comparisons, and averaged to obtain per-subject scores. 

## Reproduction Steps

1. **Clone the repo**:

   ```bash
   git clone https://github.com/grantrinehimer/dreambooth_diffusion.git
   cd <repo>
   ```
2. **Create a conda environment** (Python 3.10 recommended):

   ```bash
   conda create -n myenv python=3.10
   ```
3. **Install dependencies**:

   First perform:
   ```bash
   git clone https://github.com/huggingface/diffusers
   cd diffusers
   pip install .
   cd examples/dreambooth
   pip install -r requirements.txt
   ```
   This will get the latest version of diffusers and most required packages.
   
   ```bash
   pip install -r requirements.txt
   ```
   Note that this may cause issues depending on version.

5. **Setup Accelerate Config**

   ```bash
   accelerate config
   ```
   We recommend enabling fp16 mixed precision.

6. **Use stable_diffusion_test.ipynb to download your base pretrained model.**

   We use stable diffusion v1.5. Ensure that the model is located at the directory referred to in training_config.yaml.
   
7. **Run training script**
   ```bash
   accelerate launch batch_dreambooth.py
   ```
   This script will generate a model for every subject, so beware of disk space. It also runs inference on the fine-tuned models to generate the result images. In training_config.yaml, you can configure training parameters. We used gradient checkpointing, mixed precision FP16, and 8-bit Adam. You will also see an option to enable PPL and the number of images to generate for PPL. We used 100 images and created models both with and without PPL. Remember to change the model and images output directories in batch_config.yaml when generating PPL so as to not overwrite the models without PPL. We ran the script once with and without PPL for 400 training steps. We also used a batch size of 1.

   Note that depending on the device, it can be difficult to configure CUDA. Running `accelerate config` to use the CPU instead resolves most issues.

6. **Execute all cells in code/evaluation.ipynb**:

   * The notebook imports `pres.py`, `div.py`, and `clip_embeddings.py`.
   * It iterates over each subject & condition, computes all five metrics, and writes per-metric CSVs:
     * `pres_results.csv`
     * `div_results.csv`
     * `clip_results.csv`
     * `dino_results.csv`
   * The final cell **aggregates** these into a single summary table, saving `all_results.csv` that contains mean PRES, DIV, DINO, CLIP-I, CLIP-T for `non-ppl` and `ppl`.
   * 
## Results/Insights

Across all metrics, our reimplementation reproduces the original paper’s trends—with slightly lower absolute values owing to our reduced prompt set (8 vs. 25) and shorter fine-tuning (400 vs. 1000 steps). This is highlighted below [Table 1]; the same trends persist between the Dreambooth metrics [1] and ours. 

In both cases, adding PPL sharply reduces prior collapse (lower PRES), meaning the model no longer “hallucinates” the fine-tuned subject when generating random class samples. We interpreted this as a lack of overfitting to the original subject; PPL aids in understanding the key components of what features make up the class without recreating the original subject, which is evident when prompts contain another subject from the same class.

Moreover, PPL boosts sample diversity under both pipelines, as generated images vary more in pose, background, and articulation. Nevertheless, our significant difference in DIV scores in PPL and no-PPL of 0.245 and 0.207 respectively, are indicative of our reduced amount of output images per prompt compared to the Dreambooth [1] output (2 vs. 4): having more output images mitigates average variance. When you only have two images, that one distance completely determines the mean; with four images, you average over six distances, which smooths out any outliers and reduces the overall DIV value.

Importantly, even with only 2 outputs per prompt and 400 training steps, PPL maintained its benefits: relative reductions in PRES and gains in DIV closely match those reported by Ruiz et al. [1]. This robustness suggests that class-specific prior preservation can be deployed under constrained compute budgets without losing its ability to preserve subject identity and encourage diverse generations.

![image title](image_name.png)

## Conclusion

Fine-tuning the diffusion model with and without PPL was an excellent learning opportunity for our group. However, it is worth noting the challenges that were faced in our project. First, dealing with heavy computation can make it challenging to obtain quality, large-scale results. Just on reproducing a small subset of results from the paper, we consumed our available compute credits. Second, a related issue was that setting up our virtual machine correctly, actually using their available GPU in our code, and debugging/making adaptations to avoid out-of-memory errors during training was more than half the challenge. Unexpected considerations, such as using 8-bit Adam as a specific optimizer, were encountered during these steps. Lastly, fine-tuning can cause some seemingly magical things to happen to the subject and/or background prompt-by-prompt. It’s challenging to understand why these weird things happen, let alone fine-tune correctly. This resulted in trial and error making up a large portion of our work. 

Now, for the successes and direct conclusions of our project. We were able to reproduce the wide variety of metrics presented in the original paper, as well as the effects that using PPL had on model performance. We observed that PPL in fine-tuning increases diversity and prior class preservation. As seen in the paper, our results obtained with PPL adhere to the prompt better, but sometimes at the loss of the subject’s image quality. Additionally, the strength of a subject’s prior amplified this result. For example, inference with prompts not seen in training, such as a dog painted in a Vincent Van Gogh style, yielded higher quality results than in-dataset prompts for subjects without a strong prior, like a “dog backpack”. This leads to an important result: PPL did not increase subject preservation. Lastly, even with smaller amounts of outputs per prompt and training steps, PPL maintained its benefits. Overall, our findings are consistent with the original paper.

## References
1. Ruiz, N., Li, Y., Jampani, V., Pritch, Y., Rubinstein, M., & Aberman, K. (2023). *DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation*. arXiv:2208.12242. [https://arxiv.org/abs/2208.12242](https://arxiv.org/abs/2208.12242)
2. P. von Platen et al., Diffusers: State-of-the-art diffusion models. GitHub, 2022. [Online]. Available: https://github.com/huggingface/diffusers
3. Mathilde Caron, Hugo Touvron, Ishan Misra, Herve ́ Je ́gou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9650–9660, 2021.

## Acknowledgements
- This project was completed as part of the coursework for **CS 4782: Introduction to Deep Learning** at **Cornell University**.
- I would like to thank **Prof. Kilian Weinberger** and **Prof. Jennifer J. Sun** for their guidance throughout the semester.
- Appreciation also goes to our classmates for their insights and feedback during the final project peer review sessions on **5/1/2025** and **5/6/2025**.
